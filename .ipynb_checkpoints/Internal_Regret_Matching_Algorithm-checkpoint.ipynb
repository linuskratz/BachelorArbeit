{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1b9f69",
   "metadata": {},
   "source": [
    "# INTERNAL REGRET MATCHING ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec36400",
   "metadata": {},
   "source": [
    "## DESCRIPTION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7317cde8",
   "metadata": {},
   "source": [
    " this implementation of the internal regret matching algorithm works for NxM, 2 player games \n",
    " a game input must contain the payoffs for each player given a pair of strategies\n",
    " a game is a list; the game contains a list for each strategy of player 0, \n",
    " each list of a player 0 strategy contains a list of all player 1 strategies\n",
    " for each strategy_p0, strategy_p1 pair we include the below payoff\n",
    " a payoff looks like [player_0_payoff, player_1_payoff]\n",
    "\n",
    "Based on Hart and Mas-Colell - 2000 - A Simple Adaptive Procedure Leading to Correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2aa5e",
   "metadata": {},
   "source": [
    "## EXAMPLE GAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0636834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pennies = [[[1.0,-1.0], [-1.0, 1.0]], \n",
    "                    [[-1.0, 1.0], [1.0,-1.0]]]\n",
    "\n",
    "schere_strein_papier = [[[0,0], [-1, 1], [1, -1]], \n",
    "                        [[1,-1], [0, 0], [-1, 1]],\n",
    "                        [[-1,1], [1, -1], [0, 0]]]\n",
    "\n",
    "test_game_2 = [[[0,0], [-1, 1], [1, -1]], \n",
    "              [[1,-1], [0, 0], [-1, 1]]]\n",
    "\n",
    "prisoners_dilemma = [[[3,3], [0, 5]],\n",
    "                     [[5,0], [1, 1]]]\n",
    "\n",
    "shapley_game = [[[1,0], [0, 1], [0, 0]], \n",
    "                [[0,0], [1, 0], [0, 1]],\n",
    "                [[0,1], [0, 0], [1, 0]]]\n",
    "\n",
    "# From: No-regret Dynamics and Fictitious Play: https://hal.archives-ouvertes.fr/hal-00713871/document\n",
    "identical_interest_game = [[[2,2], [1,1],[-4,-4]],\n",
    "                          [[1,1],[0,0],[-1,-1]],\n",
    "                          [[-4,-4],[-1,-1],[-2,-2]]]\n",
    "\n",
    "# From: No-regret Dynamics and Fictitious Play: https://hal.archives-ouvertes.fr/hal-00713871/document\n",
    "eps = 0.5 \n",
    "\n",
    "coordination_game = [[[1,1],[1,1],[0,0],[0,0]],\n",
    "                    [[1-eps,1-eps],[1-eps,1-eps],[-eps,-eps],[-eps,-eps]],\n",
    "                    [[0,0],[0,0],[1,1],[1,1]], \n",
    "                    [[-eps,-eps],[-eps,-eps],[1-eps,1-eps],[1-eps,1-eps]]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58730c",
   "metadata": {},
   "source": [
    "## ALGORITHM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f3237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f4f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the number of actions for a player\n",
    "def no_of_actions (game, player_no): \n",
    "    return np.array(game).shape[player_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc11676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the utility given a pair of strategies played\n",
    "def calculate_utility_of_player(game, player, player_0_action, player_1_action): \n",
    "    return game[player_0_action][player_1_action][player]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7d8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the replacement payoff at time t of playing replacement_action_k instead of the replaced_action_j played\n",
    "# this method is only called when replaced_action_j is played\n",
    "def calculate_utility(game, player, action_player, action_other_player): \n",
    "    if player == 0: \n",
    "        utility = calculate_utility_of_player(game, player, action_player, action_other_player)\n",
    "    else: \n",
    "        utility = calculate_utility_of_player(game, player, action_other_player, action_player)    \n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cce1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_occurences(cumulative_history, player, action_player, action_other_player): \n",
    "    if player == 0: \n",
    "        occurences = cumulative_history[action_player][action_other_player]\n",
    "    else: \n",
    "        occurences = cumulative_history[action_other_player][action_player]  \n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d682ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the replacement payoff at time t of playing replacement_action_k instead of the replaced_action_j played\n",
    "# this method is only called when replaced_action_j is played\n",
    "def calculate_regret_up_to_time_2(game, time, player, cumulative_history, replaced_action_j, replacement_action_k): \n",
    "    no_actions_other_player = no_of_actions(game, (1-player))\n",
    "    cumulative_regret = 0\n",
    "    for other_player_action in range(no_actions_other_player):\n",
    "        replacement_utility = calculate_utility(game, player, replacement_action_k, other_player_action)\n",
    "        replaced_utility = calculate_utility(game, player, replaced_action_j, other_player_action)\n",
    "        regret = replacement_utility - replaced_utility\n",
    "        cumulative_regret += historical_occurences(cumulative_history, player, replaced_action_j, other_player_action) * regret\n",
    "    return max(1/time * cumulative_regret, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b36785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a new probability distribution over a players strategy set given the regret experienced in the past\n",
    "def calculate_probability_distribution_at_time_2(game, time, player, cumulative_history, my, last_chosen_action_j): \n",
    "    no_actions = no_of_actions(game, player)\n",
    "    probabilities = [(1 / my)* calculate_regret_up_to_time_2(game, time, player, cumulative_history, last_chosen_action_j, action) for action in range(no_actions) if action != last_chosen_action_j]\n",
    "    last_j_prob = 1 - sum(probabilities)\n",
    "    probabilities.insert(last_chosen_action_j, last_j_prob)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763a6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_distribution(probabilities):\n",
    "    return random.choices(range(0,len(probabilities)), weights = probabilities)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac283cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(distribution, new_distribution): \n",
    "    p0_dist = np.array(distribution[0])\n",
    "    p1_dist = np.array(distribution[1])\n",
    "    \n",
    "    p0_new_dist = np.array(new_distribution[0])\n",
    "    p1_new_dist = np.array(new_distribution[1])\n",
    "    \n",
    "    p0_distance = (p0_dist - p0_new_dist)**2\n",
    "    p1_distance = (p1_dist - p1_new_dist)**2\n",
    "    distance = sum(p0_distance) + sum(p1_distance)\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b598eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my > M^i  * (m^i -1) for all players i\n",
    "# M^i >= abs(u^i(k, s^-i)- u^i(j, s^-i)) for all j, k (maximum absolute payoff - minimum absolute payoff)\n",
    "# m^i = number of strategies player i\n",
    "# return my for each player individually \n",
    "def choose_my(game):\n",
    "    ret = []\n",
    "    for i in range(2): \n",
    "        m_i = no_of_actions(game, i)\n",
    "        flat_payoffs = np.transpose(game)[i].flatten()\n",
    "        M_i = np.absolute(np.amax(flat_payoffs) - np.amin(flat_payoffs)) \n",
    "        ret.append(M_i * (m_i-1) + 0.01)\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93f774a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_mean = old_mean + (new_value - old_mean) / step\n",
    "# https://math.stackexchange.com/questions/106700/incremental-averaging\n",
    "def update_mean_distribution(mean_distribution, update_distribution, step): \n",
    "    p0_distribution = np.array(mean_distribution[0])  \n",
    "    p1_distribution = np.array(mean_distribution[1]) \n",
    "    new_p0_mean = p0_distribution + (np.array(update_distribution[0]) - p0_distribution)/step\n",
    "    new_p1_mean = p1_distribution + (np.array(update_distribution[1]) - p1_distribution)/step\n",
    "    return [new_p0_mean.tolist(), new_p1_mean.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2070226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a random initial distribution for a game \n",
    "def initial_random_distribution(game): \n",
    "    no_of_actions_p0 = no_of_actions (game, 0)\n",
    "    no_of_actions_p1 = no_of_actions (game, 1)\n",
    "    random_dist_p0 = np.random.random_sample(no_of_actions_p0)\n",
    "    random_dist_p1 = np.random.random_sample(no_of_actions_p1)\n",
    "    #normalize \n",
    "    random_dist_p0 = random_dist_p0 / sum(random_dist_p0)\n",
    "    random_dist_p1 = random_dist_p1 / sum(random_dist_p1)\n",
    "    \n",
    "    return np.array([random_dist_p0.tolist(), random_dist_p1.tolist()], dtype = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1db02e",
   "metadata": {},
   "source": [
    "## Implementation with cumulative_history of play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b84dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroed_player_actions(game): \n",
    "    no_of_actions_p0 = no_of_actions (game, 0)\n",
    "    no_of_actions_p1 = no_of_actions (game, 1)\n",
    "    return np.array([np.zeros(no_of_actions_p0), np.zeros(no_of_actions_p1)], dtype = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae3c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the internal regret matching algorithm converges if the L2 norm change \n",
    "# in mean_distribution from step to step + 1 is smaller than epsilon\n",
    "# or when max_steps has been reached \n",
    "# the mean_distribution history is returned\n",
    "\n",
    "def internal_regret_matching_2(game, max_steps, init_distribution, my, epsilon): \n",
    "    distributions = [init_distribution]\n",
    "    action_p0 = sample_from_distribution(init_distribution[0])\n",
    "    action_p1 = sample_from_distribution(init_distribution[1])\n",
    "    cumulative_history = np.zeros(np.array(game).shape[0:2])\n",
    "    cumulative_history[action_p0][action_p1] = 1\n",
    "    history = [[action_p0, action_p1]]\n",
    "    step = 1\n",
    "    mean_distribution = init_distribution\n",
    "    while 1: \n",
    "        p0_dist = calculate_probability_distribution_at_time_2(game, step, 0, cumulative_history, my[0], action_p0)\n",
    "        p1_dist = calculate_probability_distribution_at_time_2(game, step, 1, cumulative_history, my[1], action_p1)\n",
    "        action_p0 = sample_from_distribution(p0_dist)\n",
    "        action_p1 = sample_from_distribution(p1_dist)\n",
    "        history.append([action_p0, action_p1])\n",
    "        cumulative_history[action_p0][action_p1] = cumulative_history[action_p0][action_p1] + 1\n",
    "        new_distribution = zeroed_player_actions(game)\n",
    "        new_distribution[0][action_p0] = 1\n",
    "        new_distribution[1][action_p1] = 1\n",
    "        step +=1\n",
    "        new_mean_distribution = update_mean_distribution(mean_distribution, new_distribution, step)\n",
    "        distributions.append(new_mean_distribution)\n",
    "        if euclidean_distance(mean_distribution, new_mean_distribution) <= epsilon and step >=5 or step == max_steps: \n",
    "            break\n",
    "        mean_distribution = new_mean_distribution\n",
    "    return distributions, history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964bce",
   "metadata": {},
   "source": [
    "## Implementation according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f374e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the replacement payoff at time t of playing replacement_action_k instead of the replaced_action_j played\n",
    "# this method is only called when replaced_action_j is played\n",
    "def calculate_replacement_payoff_at_time(game, time, player, history, replacement_action_k): \n",
    "    other_player_strategy = history[time][(1-player)]\n",
    "    if player == 0: \n",
    "        utility = calculate_utility_of_player(game, player, replacement_action_k, other_player_strategy)\n",
    "    else: \n",
    "        utility = calculate_utility_of_player(game, player, other_player_strategy, replacement_action_k)    \n",
    "    return utility\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b2dcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the cumulative regret of not having played action k instead of action j in the past  \n",
    "def calculate_regret_up_to_time(game, time, player, history, replaced_action_j, replacement_action_k):\n",
    "    regret = 0 \n",
    "    for step in range(time): \n",
    "        if history[step][player] == replaced_action_j:\n",
    "            original_payoff = calculate_utility_of_player(game, player, history[step][0],  history[step][1])\n",
    "            replacement_payoff = calculate_replacement_payoff_at_time(game, step, player, history, replacement_action_k)\n",
    "            regret += (replacement_payoff - original_payoff)\n",
    "    return max(1/time * regret, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f07697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a new probability distribution over a players strategy set given the regret experienced in the past\n",
    "def calculate_probability_distribution_at_time(game, time, player, history, my): \n",
    "    last_chosen_action_j = history[(time-1)][player]\n",
    "    no_actions = no_of_actions(game, player)\n",
    "    probabilities = [(1 / my)* calculate_regret_up_to_time(game, time, player, history, last_chosen_action_j, action) for action in range(no_actions) if action != last_chosen_action_j]\n",
    "    last_j_prob = 1 - sum(probabilities)\n",
    "    probabilities.insert(last_chosen_action_j, last_j_prob)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46795dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the internal regret matching algorithm converges if the L2 norm change \n",
    "# in mean_distribution from step to step + 1 is smaller than epsilon\n",
    "# or when max_steps has been reached \n",
    "# the mean_distribution history is returned\n",
    "\n",
    "def internal_regret_matching(game, max_steps, init_distribution, my, epsilon): \n",
    "    distributions = [init_distribution]\n",
    "    history_p0 = sample_from_distribution(init_distribution[0])\n",
    "    history_p1 = sample_from_distribution(init_distribution[1])\n",
    "    history = [[history_p0,history_p1]]\n",
    "    step = 1\n",
    "    mean_distribution = init_distribution\n",
    "    while 1: \n",
    "        p0_dist = calculate_probability_distribution_at_time(game, step, 0, history, my[0])\n",
    "        p1_dist = calculate_probability_distribution_at_time(game, step, 1, history, my[1])\n",
    "        new_hist = [sample_from_distribution(p0_dist), sample_from_distribution(p1_dist)]\n",
    "        history.append(new_hist)\n",
    "        new_distribution = zeroed_player_actions(game)\n",
    "        for i in range(2):\n",
    "            np.put(new_distribution[i], new_hist[i], 1.0)\n",
    "        step +=1\n",
    "        new_mean_distribution = update_mean_distribution(mean_distribution, new_distribution, step)\n",
    "        distributions.append(new_mean_distribution)\n",
    "        if euclidean_distance(mean_distribution, new_mean_distribution) <= epsilon and step >=5 or step == max_steps: \n",
    "            break\n",
    "        mean_distribution = new_mean_distribution\n",
    "    return distributions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940ab91",
   "metadata": {},
   "source": [
    "## Functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec4aeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history(player_no, history): \n",
    "    return [strategy[player_no] for strategy in history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a75b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots a history of the mean distribution for each action of each player\n",
    "\n",
    "def plot_history(history, game_name): \n",
    "    \n",
    "    hist_p0 = np.transpose(get_history(0, history))\n",
    "    hist_p1 = np.transpose(get_history(1, history))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for action_no, action_history_p0 in enumerate(hist_p0):\n",
    "        ax.plot(range(len(action_history_p0)), action_history_p0, label =\"player:0, action:{}\".format(action_no))\n",
    "    \n",
    "    for action_no, action_history_p1 in enumerate(hist_p1):\n",
    "        ax.plot(range(len(action_history_p1)), action_history_p1, label =\"player:1, action:{}\".format(action_no))\n",
    "    \n",
    "    ax.set_title(\"Internal Regret Matching algorithm with {}\".format(game_name))\n",
    "    ax.set_xlabel('Number of iterations')\n",
    "    ax.set_ylabel('Averaged strategy probability\\nover number of iterations played')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb30eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_regret_matching_plot(game, game_name, init_distribution, max_steps = 100000, epsilon = 0.00001):\n",
    "    my = choose_my(game)\n",
    "    distributions, history = internal_regret_matching_2(game, max_steps, init_distribution, my, epsilon)\n",
    "    plot_history(distributions, game_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25590f",
   "metadata": {},
   "source": [
    "# FINDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1b54aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#internal_regret_matching_plot(prisoners_dilemma, \"prisoners_dilemma\", initial_random_distribution(prisoners_dilemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "617bc734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#internal_regret_matching_plot(matching_pennies, \"matching_pennies\", initial_random_distribution(matching_pennies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aff7c2b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#internal_regret_matching_plot(schere_strein_papier, \"schere_strein_papier\", initial_random_distribution(schere_strein_papier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad140f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#internal_regret_matching_plot(shapley_game, \"shapley_game\", initial_random_distribution(shapley_game))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18f054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
